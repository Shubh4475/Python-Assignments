{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**THEORETICAL**"
      ],
      "metadata": {
        "id": "V1Z7XJ8umWcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is a Decision Tree, and how does it work?**"
      ],
      "metadata": {
        "id": "q5kX3yfEmeD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** A Decision Tree is a popular machine learning algorithm used for classification and regression tasks. It works like a flowchart, where each internal node represents a decision based on a specific feature, branches represent possible outcomes, and leaf nodes represent final predictions or classifications.\n",
        "\n",
        "### **How It Works**\n",
        "1. **Splitting the Data** – The algorithm selects the best feature to split the dataset based on a criterion like **Gini impurity** or **entropy** (for classification) and **Mean Squared Error (MSE)** (for regression).\n",
        "2. **Recursive Partitioning** – The dataset is divided into subsets based on the selected feature, and the process continues recursively, forming a tree structure.\n",
        "3. **Stopping Criteria** – The tree stops growing when a predefined condition is met (e.g., maximum depth reached, minimum number of samples per node, or when further splitting does not improve the model).\n",
        "4. **Prediction** – When making a prediction, the input follows the tree's paths from the root to a leaf node, determining the final decision.\n",
        "\n",
        "### **Advantages**\n",
        "- Easy to interpret and visualize.\n",
        "- Handles both numerical and categorical data.\n",
        "- Requires minimal data preprocessing.\n",
        "\n",
        "### **Disadvantages**\n",
        "- Prone to overfitting, especially with deep trees.\n",
        "- Sensitive to noisy data.\n",
        "\n"
      ],
      "metadata": {
        "id": "91UsH69poHuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What are impurity measures in Decision Trees?**"
      ],
      "metadata": {
        "id": "x7VonJW0mnET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** In Decision Trees, **impurity measures** are metrics used to determine how \"mixed\" or \"impure\" a node is in terms of the class labels of the samples it contains. They guide the **splitting process** of the tree—helping decide which feature and threshold to use at each step to best separate the classes.\n",
        "\n",
        "The goal is to **reduce impurity** with each split, ideally ending up with pure (or nearly pure) nodes that contain samples of only one class.\n",
        "\n",
        "### Common Impurity Measures:\n",
        "\n",
        "#### 1. **Gini Impurity**\n",
        "\n",
        "* **Formula:**\n",
        "\n",
        "  $$\n",
        "  Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "  $$\n",
        "\n",
        "  where $p_i$ is the proportion of instances of class $i$ in the node, and $C$ is the number of classes.\n",
        "\n",
        "* **Range:** 0 (pure) to \\~0.5 (most impure in binary classification)\n",
        "\n",
        "* **Used in:** CART (Classification and Regression Trees)\n",
        "\n",
        "#### 2. **Entropy (Information Gain)**\n",
        "\n",
        "* **Formula:**\n",
        "\n",
        "  $$\n",
        "  Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "  $$\n",
        "\n",
        "* **Information Gain** is the decrease in entropy after a split.\n",
        "\n",
        "  $$\n",
        "  IG = Entropy(parent) - \\text{Weighted Average of Entropy(children)}\n",
        "  $$\n",
        "\n",
        "* **Range:** 0 (pure) to 1 (most impure in binary classification)\n",
        "\n",
        "* **Used in:** ID3, C4.5 algorithms\n",
        "\n",
        "#### 3. **Classification Error (Misclassification Rate)**\n",
        "\n",
        "* **Formula:**\n",
        "\n",
        "  $$\n",
        "  Error = 1 - \\max(p_i)\n",
        "  $$\n",
        "\n",
        "  where $\\max(p_i)$ is the proportion of the majority class in the node.\n",
        "\n",
        "* **Range:** 0 (pure) to 0.5 (impure in binary classification)\n",
        "\n",
        "* **Less sensitive** to changes in node class distribution, so less often used for splits but sometimes used for pruning.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Comparison\n",
        "\n",
        "| Measure              | Sensitive to Class Distribution? | Used For Splitting? |\n",
        "| -------------------- | -------------------------------- | ------------------- |\n",
        "| Gini Impurity        | Yes                              | Yes                 |\n",
        "| Entropy              | Yes                              | Yes                 |\n",
        "| Classification Error | Less so                          | Rarely              |\n",
        "\n",
        "Each measure quantifies impurity differently, but the general principle is the same: choose splits that **reduce impurity** the most.\n"
      ],
      "metadata": {
        "id": "1ZtTmZ3moVRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What is the mathematical formula for Gini Impurity?**"
      ],
      "metadata": {
        "id": "WVel_6Tnmsld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** The **mathematical formula for Gini Impurity** at a node is:\n",
        "\n",
        "$$\n",
        "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "$$\n",
        "\n",
        "### Where:\n",
        "\n",
        "* $C$ is the number of classes,\n",
        "* $p_i$ is the proportion (probability) of samples belonging to class $i$ at that node.\n",
        "\n",
        "---\n",
        "\n",
        "### Example (Binary Classification):\n",
        "\n",
        "If a node contains:\n",
        "\n",
        "* 4 samples of class A\n",
        "* 6 samples of class B\n",
        "\n",
        "Then:\n",
        "\n",
        "* $p_A = \\frac{4}{10} = 0.4$\n",
        "* $p_B = \\frac{6}{10} = 0.6$\n",
        "\n",
        "$$\n",
        "Gini = 1 - (0.4^2 + 0.6^2) = 1 - (0.16 + 0.36) = 1 - 0.52 = 0.48\n",
        "$$\n",
        "\n",
        "A **Gini Impurity of 0** means the node is **pure** (only one class is present). The closer it is to 0, the better the split.\n"
      ],
      "metadata": {
        "id": "xmKTXFGCrIAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What is the mathematical formula for Entropy?**"
      ],
      "metadata": {
        "id": "waPRw3KVm5Im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans**: The **mathematical formula for Entropy** at a node (used in Decision Trees) is:\n",
        "\n",
        "$$\n",
        "Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "### Where:\n",
        "\n",
        "* $C$ is the number of classes,\n",
        "* $p_i$ is the proportion (probability) of samples belonging to class $i$ at that node,\n",
        "* The logarithm is base 2 (to measure entropy in bits).\n",
        "\n",
        "---\n",
        "\n",
        "### Important Notes:\n",
        "\n",
        "* If $p_i = 0$ for any class, the term $p_i \\log_2(p_i)$ is defined as 0 (since $\\lim_{p \\to 0} p \\log_2(p) = 0$).\n",
        "* **Entropy = 0** when the node is pure (i.e., all samples belong to one class).\n",
        "* The **maximum entropy** occurs when the classes are equally likely.\n",
        "\n",
        "---\n",
        "\n",
        "### Example (Binary Classification):\n",
        "\n",
        "If a node has:\n",
        "\n",
        "* 50% class A → $p_A = 0.5$\n",
        "* 50% class B → $p_B = 0.5$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "Entropy = -(0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = -2 \\times 0.5 \\times (-1) = 1\n",
        "$$\n",
        "\n",
        "This represents the **maximum entropy** for a binary classification, indicating maximum impurity (complete uncertainty).\n"
      ],
      "metadata": {
        "id": "uvSDoUSirVf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5 What is Information Gain, and how is it used in Decision Trees?**"
      ],
      "metadata": {
        "id": "yzfHcs6lnBQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** **Information Gain (IG)** is a metric used in **Decision Trees** to quantify the **reduction in entropy (uncertainty or impurity)** achieved by splitting a dataset based on a particular feature.\n",
        "\n",
        "It helps in selecting the **best feature to split** the data at each node in the tree.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔢 **Mathematical Definition**\n",
        "\n",
        "$$\n",
        "\\text{Information Gain} = Entropy(\\text{parent}) - \\sum_{k=1}^{K} \\frac{N_k}{N} \\cdot Entropy(\\text{child}_k)\n",
        "$$\n",
        "\n",
        "### Where:\n",
        "\n",
        "* $Entropy(\\text{parent})$: Entropy before the split.\n",
        "* $K$: Number of child nodes after the split.\n",
        "* $N$: Total number of samples in the parent node.\n",
        "* $N_k$: Number of samples in child node $k$.\n",
        "* $Entropy(\\text{child}_k)$: Entropy of child node $k$.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **How It’s Used in Decision Trees:**\n",
        "\n",
        "1. For each feature, try all possible splits.\n",
        "2. For each split, compute the **Information Gain**.\n",
        "3. Select the **split with the highest Information Gain**.\n",
        "4. Repeat recursively on resulting child nodes.\n",
        "\n",
        "This process continues until a stopping condition is met (e.g., pure nodes, max depth, minimum samples).\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Example:\n",
        "\n",
        "Suppose a parent node has:\n",
        "\n",
        "* 10 samples: 5 of class A and 5 of class B\n",
        "* $Entropy = 1$ (maximum uncertainty)\n",
        "\n",
        "You split into:\n",
        "\n",
        "* Left node: 4 of class A, 1 of class B → $Entropy \\approx 0.72$\n",
        "* Right node: 1 of class A, 4 of class B → $Entropy \\approx 0.72$\n",
        "\n",
        "Weighted child entropy:\n",
        "\n",
        "$$\n",
        "\\frac{5}{10} \\cdot 0.72 + \\frac{5}{10} \\cdot 0.72 = 0.72\n",
        "$$\n",
        "\n",
        "Information Gain:\n",
        "\n",
        "$$\n",
        "1 - 0.72 = 0.28\n",
        "$$\n",
        "\n",
        "This means the split **reduces uncertainty** by 0.28.\n",
        "\n",
        "---\n",
        "\n",
        "### 📘 Summary:\n",
        "\n",
        "* **Information Gain** helps build decision trees by choosing the most informative splits.\n",
        "* It is based on **Entropy**, so it's commonly used in algorithms like **ID3** and **C4.5**.\n",
        "* A higher Information Gain means a **more effective split**.\n"
      ],
      "metadata": {
        "id": "tLSf26M7re1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. What is the difference between Gini Impurity and Entropy?**"
      ],
      "metadata": {
        "id": "EYmCNGT4nGGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** The **main difference between Gini Impurity and Entropy** lies in how they measure the impurity of a node in a Decision Tree and how they respond to class distributions. Both are used to decide the best feature for splitting the data, but they have **different formulas and properties**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔢 **1. Mathematical Formulas**\n",
        "\n",
        "**Gini Impurity:**\n",
        "\n",
        "$$\n",
        "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "$$\n",
        "\n",
        "**Entropy:**\n",
        "\n",
        "$$\n",
        "Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $C$ is the number of classes,\n",
        "* $p_i$ is the proportion of class $i$ at the node.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚖️ **2. Range of Values**\n",
        "\n",
        "| Measure       | Minimum (Pure Node) | Maximum (Binary Classes) |\n",
        "| ------------- | ------------------- | ------------------------ |\n",
        "| Gini Impurity | 0                   | 0.5                      |\n",
        "| Entropy       | 0                   | 1                        |\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 **3. Sensitivity to Class Distribution**\n",
        "\n",
        "* **Entropy** is **more sensitive** to changes in class probabilities, especially when the classes are closer in frequency.\n",
        "* **Gini Impurity** tends to be **less sensitive**, often favoring the most frequent class more strongly.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 **4. Interpretation**\n",
        "\n",
        "* **Entropy** comes from information theory and measures the amount of **information (or surprise)** in a distribution.\n",
        "* **Gini** is more of a **probability-based measure**, representing the chance of incorrect classification.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 **5. Practical Differences**\n",
        "\n",
        "| Aspect                   | Gini Impurity         | Entropy                |\n",
        "| ------------------------ | --------------------- | ---------------------- |\n",
        "| Faster to compute        | ✅ Yes (no logs)       | ❌ No (uses log base 2) |\n",
        "| Used in                  | CART algorithm        | ID3, C4.5 algorithms   |\n",
        "| Bias toward larger class | Slightly higher       | More balanced          |\n",
        "| Typical performance      | Similar in most cases | Similar in most cases  |\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ **Summary**\n",
        "\n",
        "| Feature     | Gini Impurity    | Entropy                         |\n",
        "| ----------- | ---------------- | ------------------------------- |\n",
        "| Formula     | $1 - \\sum p_i^2$ | $-\\sum p_i \\log_2 p_i$          |\n",
        "| Used In     | CART             | ID3, C4.5                       |\n",
        "| Computation | Faster           | Slightly slower (uses log)      |\n",
        "| Sensitivity | Less sensitive   | More sensitive to class balance |\n",
        "\n",
        "Both are valid and often yield **similar trees** in practice, but Gini is commonly preferred in performance-critical applications like **scikit-learn**'s implementation of CART.\n"
      ],
      "metadata": {
        "id": "1kby-94mryHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. What is the mathematical explanation behind Decision Trees?**"
      ],
      "metadata": {
        "id": "4wxJRgAunP3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** The **mathematical explanation behind Decision Trees** is grounded in **recursive partitioning of the feature space** to reduce a measure of impurity (e.g., Gini, Entropy, or variance for regression). Here's a breakdown of how Decision Trees work mathematically:\n",
        "\n",
        "---\n",
        "\n",
        "## 🔢 1. **Dataset and Problem Setup**\n",
        "\n",
        "Let the training data be:\n",
        "\n",
        "$$\n",
        "D = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N}\n",
        "$$\n",
        "\n",
        "* $x^{(i)} \\in \\mathbb{R}^d$: Feature vector of the $i$-th sample.\n",
        "* $y^{(i)}$: Target value (class label for classification or numeric value for regression).\n",
        "\n",
        "The goal is to partition the data **recursively** into subsets that are as **pure** as possible.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 2. **Recursive Splitting**\n",
        "\n",
        "At each node:\n",
        "\n",
        "* Consider all features $x_j$ and possible split points $s$.\n",
        "* For each candidate split, partition $D$ into:\n",
        "\n",
        "  * $D_{\\text{left}} = \\{x \\in D \\mid x_j \\le s\\}$\n",
        "  * $D_{\\text{right}} = \\{x \\in D \\mid x_j > s\\}$\n",
        "\n",
        "Compute the **impurity before and after** the split.\n",
        "\n",
        "---\n",
        "\n",
        "## 📉 3. **Impurity Measures**\n",
        "\n",
        "### For **classification**, typical impurity measures:\n",
        "\n",
        "* **Gini Impurity:**\n",
        "\n",
        "  $$\n",
        "  Gini(D) = 1 - \\sum_{k=1}^{C} p_k^2\n",
        "  $$\n",
        "* **Entropy:**\n",
        "\n",
        "  $$\n",
        "  Entropy(D) = -\\sum_{k=1}^{C} p_k \\log_2(p_k)\n",
        "  $$\n",
        "\n",
        "Where $p_k$ is the proportion of class $k$ in node $D$.\n",
        "\n",
        "### For **regression**, use **variance** or **mean squared error**:\n",
        "\n",
        "$$\n",
        "Var(D) = \\frac{1}{|D|} \\sum_{i=1}^{|D|} (y^{(i)} - \\bar{y})^2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ➕ 4. **Split Evaluation**\n",
        "\n",
        "For a candidate split:\n",
        "\n",
        "$$\n",
        "\\text{Impurity}_{\\text{split}} = \\frac{|D_{\\text{left}}|}{|D|} \\cdot I(D_{\\text{left}}) + \\frac{|D_{\\text{right}}|}{|D|} \\cdot I(D_{\\text{right}})\n",
        "$$\n",
        "\n",
        "Where $I(\\cdot)$ is the impurity function (e.g., Gini, Entropy).\n",
        "\n",
        "### Choose the split that **minimizes** this weighted impurity:\n",
        "\n",
        "$$\n",
        "\\text{Best split} = \\arg\\min_{j,s} \\text{Impurity}_{\\text{split}}(j, s)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 🌳 5. **Tree Construction Algorithm (Simplified)**\n",
        "\n",
        "1. Start with all data at the root node.\n",
        "2. For each node:\n",
        "\n",
        "   * If the node is pure or stopping criteria is met (e.g., depth, min samples), stop.\n",
        "   * Else, find the best split by minimizing impurity.\n",
        "   * Partition the data and repeat recursively on child nodes.\n",
        "3. Assign a prediction to each **leaf node**:\n",
        "\n",
        "   * Classification: majority class.\n",
        "   * Regression: average target value.\n",
        "\n",
        "---\n",
        "\n",
        "## 📘 Summary (Mathematical Workflow)\n",
        "\n",
        "| Step                   | Mathematics                           |\n",
        "| ---------------------- | ------------------------------------- |\n",
        "| Candidate splits       | $(x_j \\le s)$                         |\n",
        "| Impurity calculation   | Gini, Entropy, or Variance            |\n",
        "| Split evaluation       | Weighted impurity after split         |\n",
        "| Optimization objective | $\\min \\text{Impurity}_{\\text{split}}$ |\n",
        "| Recursion              | Repeat until stopping condition       |\n",
        "\n",
        "---\n",
        "\n",
        "This process forms a **piecewise decision function** over the input space that segments it into regions with similar target values or labels.\n"
      ],
      "metadata": {
        "id": "KpHcfIKRsE9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. What is Pre-Pruning in Decision Tree?**"
      ],
      "metadata": {
        "id": "7bHQZnW1nWli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** **Pre-pruning** (also known as **early stopping**) is a technique used in **Decision Trees** to **halt the tree growth early**, before it becomes overly complex and overfits the training data.\n",
        "\n",
        "---\n",
        "\n",
        "## 🌱 **What is Pre-Pruning?**\n",
        "\n",
        "In pre-pruning, the algorithm **stops splitting a node during tree construction** if certain conditions are met, **even if the node is not pure**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 **Why Pre-Pruning?**\n",
        "\n",
        "* To **prevent overfitting** by controlling the tree's size and complexity.\n",
        "* To improve **generalization** on unseen data.\n",
        "* To **reduce training time** and computational cost.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ **Common Pre-Pruning Criteria**\n",
        "\n",
        "The tree stops growing when any of these conditions are met:\n",
        "\n",
        "| Criterion                              | Description                                                                                                              |\n",
        "| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Max Depth**                          | Limit the maximum depth of the tree.                                                                                     |\n",
        "| **Min Samples per Node**               | Stop splitting if a node has fewer than a specified number of samples (e.g., `min_samples_split` or `min_samples_leaf`). |\n",
        "| **Max Number of Nodes**                | Set a limit on the total number of nodes in the tree.                                                                    |\n",
        "| **Minimum Information Gain**           | Stop if the best split doesn't reduce impurity enough (e.g., `min_impurity_decrease`).                                   |\n",
        "| **Max Features**                       | Restrict the number of features to consider at each split.                                                               |\n",
        "| **Chi-squared test (in ID3 variants)** | Stop if a statistical test shows no significant gain.                                                                    |\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **Example (Min Samples Split)**\n",
        "\n",
        "If you set:\n",
        "\n",
        "```python\n",
        "min_samples_split = 10\n",
        "```\n",
        "\n",
        "Then the tree will **not split** a node if it has **fewer than 10 samples**, even if the split would improve purity.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔄 **Pre-Pruning vs Post-Pruning**\n",
        "\n",
        "| Aspect       | Pre-Pruning                       | Post-Pruning                    |\n",
        "| ------------ | --------------------------------- | ------------------------------- |\n",
        "| When applied | During tree construction          | After full tree is built        |\n",
        "| Purpose      | Prevent overfitting early         | Simplify a fully grown tree     |\n",
        "| Risk         | May stop too early (underfitting) | May be more accurate but slower |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Summary\n",
        "\n",
        "**Pre-Pruning** is an efficient strategy to **limit tree growth during training** using predefined constraints (like depth or sample count). It strikes a balance between model complexity and performance, helping avoid overfitting and improving generalization.\n"
      ],
      "metadata": {
        "id": "ZViMZ0WPsW0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. What is Post-Pruning in Decision Trees?**"
      ],
      "metadata": {
        "id": "yDM1HxJOnbRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** **Post-pruning** (also known as **cost-complexity pruning** or **error-based pruning**) is a technique used in **Decision Trees** to simplify a **fully grown tree** by **removing branches that have little importance**, aiming to improve generalization and reduce overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "## 🌳 What Is Post-Pruning?\n",
        "\n",
        "* After growing a **complete tree** (often to pure leaves or maximum allowed size), post-pruning **examines each subtree** from the bottom up.\n",
        "* It **replaces subtrees with leaf nodes** if doing so **improves or maintains prediction accuracy** on a validation set or based on a complexity penalty.\n",
        "\n",
        "---\n",
        "\n",
        "## 📉 Why Use Post-Pruning?\n",
        "\n",
        "* Fully grown trees tend to **overfit** by modeling noise or outliers.\n",
        "* Post-pruning helps create a **simpler, more robust model** that performs better on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔢 Common Post-Pruning Methods\n",
        "\n",
        "### 1. **Reduced Error Pruning**\n",
        "\n",
        "* Evaluate the tree on a **validation set**.\n",
        "* For each non-leaf node, consider **replacing it with a leaf** (majority class).\n",
        "* Keep the change **if it doesn't increase error** on the validation set.\n",
        "* Repeat recursively.\n",
        "\n",
        "### 2. **Cost-Complexity Pruning (used in CART)**\n",
        "\n",
        "* Introduce a **penalty term** for tree complexity:\n",
        "\n",
        "  $$\n",
        "  R_\\alpha(T) = R(T) + \\alpha \\cdot |T|\n",
        "  $$\n",
        "\n",
        "  * $R(T)$: Error rate (e.g., misclassification or MSE).\n",
        "  * $|T|$: Number of leaves (complexity).\n",
        "  * $\\alpha$: Complexity parameter.\n",
        "* Prune subtrees to minimize $R_\\alpha(T)$.\n",
        "* This produces a **sequence of simpler trees**, and cross-validation is used to choose the best one.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔄 Pre-Pruning vs Post-Pruning\n",
        "\n",
        "| Feature      | Pre-Pruning                         | Post-Pruning                     |\n",
        "| ------------ | ----------------------------------- | -------------------------------- |\n",
        "| When applied | During tree building                | After tree is fully built        |\n",
        "| Risk         | May stop too early (underfitting)   | Can overfit first, then correct  |\n",
        "| Evaluation   | Uses fixed thresholds (e.g., depth) | Uses validation or penalty score |\n",
        "| Flexibility  | Less flexible                       | More flexible                    |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Summary\n",
        "\n",
        "**Post-pruning** simplifies an overfitted decision tree **after it's fully grown** by removing branches that do not improve generalization. It is a powerful method to reduce model complexity while preserving or improving predictive accuracy. It's widely used in algorithms like **CART** and provides a **more data-driven, systematic approach** than pre-pruning.\n"
      ],
      "metadata": {
        "id": "NFYmTU5Hsnmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. What is the difference between Pre-Pruning and Post-Pruning?**"
      ],
      "metadata": {
        "id": "6VhoJ6rangCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** The difference between **Pre-Pruning** and **Post-Pruning** in Decision Trees lies in **when** and **how** the tree is simplified to avoid overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔄 **Pre-Pruning vs Post-Pruning: Side-by-Side Comparison**\n",
        "\n",
        "| Feature                | **Pre-Pruning** (Early Stopping)                          | **Post-Pruning** (Simplification after Full Growth)           |\n",
        "| ---------------------- | --------------------------------------------------------- | ------------------------------------------------------------- |\n",
        "| **When applied**       | During tree construction                                  | After the full tree is built                                  |\n",
        "| **Stopping criterion** | Uses predefined thresholds (e.g., max depth, min samples) | Evaluates performance on validation set or complexity penalty |\n",
        "| **Risk**               | Can **underfit** by stopping too early                    | Can **overfit first**, then correct by pruning                |\n",
        "| **Flexibility**        | Less flexible (hard-coded constraints)                    | More flexible (evaluates real tree performance)               |\n",
        "| **Evaluation method**  | Heuristic thresholds                                      | Empirical evaluation or cost-complexity pruning               |\n",
        "| **Efficiency**         | Faster, since it avoids building full tree                | Slower, builds full tree first, then prunes                   |\n",
        "| **Used in**            | Often in fast, large-scale systems                        | Common in algorithms like CART, C4.5                          |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 **Analogy**\n",
        "\n",
        "* **Pre-Pruning** is like **stopping a sculptor early** before they finish the statue.\n",
        "* **Post-Pruning** is like **finishing the statue fully, then chipping away unnecessary details**.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Summary**\n",
        "\n",
        "| Term     | Pre-Pruning               | Post-Pruning                         |\n",
        "| -------- | ------------------------- | ------------------------------------ |\n",
        "| Goal     | Prevent overfitting early | Remove overfitting after full growth |\n",
        "| Strength | Simple, fast              | Data-driven, effective               |\n",
        "| Weakness | May underfit              | May be computationally heavier       |\n",
        "\n",
        "In practice, **post-pruning is often more accurate** but **pre-pruning is faster** and easier to control. Some tree algorithms (like `CART` in scikit-learn) support both via parameters like `max_depth` (pre-pruning) and `ccp_alpha` (post-pruning).\n"
      ],
      "metadata": {
        "id": "mhMhl6c_s1h4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11. What is a Decision Tree Regressor?**"
      ],
      "metadata": {
        "id": "wWzDoj4cnln6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** A **Decision Tree Regressor** is a type of decision tree used for **regression tasks**, where the goal is to **predict a continuous (numeric) value** rather than a class label.\n",
        "\n",
        "---\n",
        "\n",
        "## 🌳 **What Is a Decision Tree Regressor?**\n",
        "\n",
        "It builds a **tree structure** that recursively splits the input space into regions, and at each **leaf node**, it outputs a **numeric prediction**, typically the **mean** of the target values in that region.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔢 **How It Works (Mathematically)**\n",
        "\n",
        "At each split:\n",
        "\n",
        "* Choose the feature $x_j$ and threshold $s$ that minimizes the **Mean Squared Error (MSE)**:\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)} - \\hat{y})^2\n",
        "$$\n",
        "\n",
        "* For a split:\n",
        "\n",
        "  * Left node: $x_j \\leq s$\n",
        "  * Right node: $x_j > s$\n",
        "\n",
        "The split is chosen to minimize the **weighted average MSE** of the child nodes:\n",
        "\n",
        "$$\n",
        "\\text{Split MSE} = \\frac{N_L}{N} \\cdot MSE_L + \\frac{N_R}{N} \\cdot MSE_R\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $N_L, N_R$: Number of samples in left and right nodes.\n",
        "* $MSE_L, MSE_R$: MSE in each child.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Prediction Rule**\n",
        "\n",
        "Once the tree is trained:\n",
        "\n",
        "* For a new input $x$, follow the decision rules down the tree.\n",
        "* The prediction is the **mean target value** of the samples in the reached leaf.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **Key Features**\n",
        "\n",
        "| Feature                 | Description                            |\n",
        "| ----------------------- | -------------------------------------- |\n",
        "| **Output**              | Continuous value (e.g., price, age)    |\n",
        "| **Splitting criterion** | Minimizes variance or MSE              |\n",
        "| **Prediction**          | Mean of values in a leaf node          |\n",
        "| **Non-linear**          | Can model complex, non-linear patterns |\n",
        "| **Interpretability**    | Easy to visualize and interpret        |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 **Example**\n",
        "\n",
        "Predicting house prices based on features like:\n",
        "\n",
        "* Size\n",
        "* Location\n",
        "* Number of bedrooms\n",
        "\n",
        "The tree will partition the data into regions (e.g., houses with size > 1500 sqft and location = urban) and assign each region an average price based on training data.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 **In Code (scikit-learn)**\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "model = DecisionTreeRegressor(max_depth=3)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "prediction = model.predict(X_test)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 📘 Summary\n",
        "\n",
        "A **Decision Tree Regressor** is a tree-based model that predicts **continuous values** by **recursively partitioning** the input space and averaging the outcomes in each region. It's intuitive, handles non-linearity well, and forms the basis for powerful ensemble methods like **Random Forests** and **Gradient Boosting Machines (GBMs)**.\n"
      ],
      "metadata": {
        "id": "9y3vMl_ltDG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12. What are the advantages and disadvantages of Decision Trees?**"
      ],
      "metadata": {
        "id": "CDdgioV0nqZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** Here’s a comprehensive overview of the **advantages and disadvantages** of **Decision Trees**:\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Advantages of Decision Trees**\n",
        "\n",
        "| Advantage                      | Description                                                                  |\n",
        "| ------------------------------ | ---------------------------------------------------------------------------- |\n",
        "| **Easy to understand**         | Intuitive model; can be visualized as a flowchart/tree structure.            |\n",
        "| **No feature scaling needed**  | No need to normalize or standardize data (unlike SVM, KNN, etc.).            |\n",
        "| **Handles both types of data** | Can handle numerical and categorical features.                               |\n",
        "| **Non-linear relationships**   | Can model complex, non-linear patterns without transformation.               |\n",
        "| **Feature selection built-in** | Automatically selects important features by splitting.                       |\n",
        "| **Little data preparation**    | Doesn’t require dummy encoding (for tree algorithms that handle categories). |\n",
        "| **Fast to train and predict**  | Especially with small to medium datasets.                                    |\n",
        "| **Can handle missing values**  | Some implementations can work with incomplete data.                          |\n",
        "\n",
        "---\n",
        "\n",
        "## ❌ **Disadvantages of Decision Trees**\n",
        "\n",
        "| Disadvantage                         | Description                                                                        |\n",
        "| ------------------------------------ | ---------------------------------------------------------------------------------- |\n",
        "| **Overfitting**                      | Prone to creating overly complex trees that fit noise in the training data.        |\n",
        "| **Unstable**                         | Small changes in data can lead to different trees (high variance).                 |\n",
        "| **Biased splits**                    | Can favor features with more levels (especially with categorical features).        |\n",
        "| **Greedy splitting**                 | Uses a local, greedy strategy at each node—may miss globally optimal trees.        |\n",
        "| **Poor generalization (deep trees)** | Deep trees may generalize poorly unless pruned.                                    |\n",
        "| **Not smooth**                       | Creates piecewise constant approximations (no smooth curves).                      |\n",
        "| **Limited extrapolation**            | Doesn’t predict well beyond the range of training data (especially in regression). |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 **When to Use Decision Trees**\n",
        "\n",
        "* When interpretability and **visual explanation** are important.\n",
        "* As a **baseline model** before trying more complex methods.\n",
        "* In **ensembles** like Random Forests or Gradient Boosting to mitigate weaknesses.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 **Summary**\n",
        "\n",
        "| Decision Trees                                |\n",
        "| --------------------------------------------- |\n",
        "| ✅ Easy to use and interpret                   |\n",
        "| ✅ Handles both data types                     |\n",
        "| ❌ Can overfit and be unstable                 |\n",
        "| ❌ Not ideal alone for large, complex datasets |\n",
        "\n",
        "Used wisely, especially with pruning or in ensembles, **Decision Trees** can be **powerful and flexible tools** for both classification and regression.\n"
      ],
      "metadata": {
        "id": "S56gwK-AtRnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13. How does a Decision Tree handle missing values?**"
      ],
      "metadata": {
        "id": "qjtmrI7LnvJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** Decision Trees can handle **missing values** using several techniques, depending on the algorithm and implementation. Here's how they manage it:\n",
        "\n",
        "---\n",
        "\n",
        "## 🌳 **1. Ignoring Missing Values During Splitting (Surrogate Splits)**\n",
        "\n",
        "* If a feature value is missing at a node during training or prediction, the tree can use a **surrogate split**.\n",
        "* A **surrogate split** is an alternative feature that **mimics the behavior** of the primary split.\n",
        "\n",
        "### Example:\n",
        "\n",
        "* If the best split is `Feature A ≤ 5`, but A is missing for some samples,\n",
        "* The tree may use `Feature B ≤ 2` **as a proxy** if it closely approximates the same division.\n",
        "\n",
        "**Used in:** CART (Classification and Regression Trees), C5.0\n",
        "\n",
        "---\n",
        "\n",
        "## 🔀 **2. Splitting Based on Available Features Only**\n",
        "\n",
        "* During training, the split is made **only using samples where the feature is present**.\n",
        "* Samples with missing values are either:\n",
        "\n",
        "  * **Ignored** for that specific split.\n",
        "  * Or assigned **proportionally** to branches based on training distribution.\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 **3. Imputing Missing Values**\n",
        "\n",
        "Before feeding data into the tree, **missing values can be filled** using:\n",
        "\n",
        "* **Mean/Median** (for numerical features)\n",
        "* **Mode** (for categorical features)\n",
        "* **More advanced techniques** like KNN imputation or regression imputation.\n",
        "\n",
        "**Note:** This is a **preprocessing step**, not handled by the tree itself.\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ **4. Treating “Missing” as a Separate Category**\n",
        "\n",
        "For categorical features, some algorithms treat \"missing\" as its **own category**.\n",
        "\n",
        "* For example: \\[Red, Green, Missing] is handled like any other category.\n",
        "\n",
        "This works best if the missingness itself is **informative**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📦 **Handling in Popular Libraries**\n",
        "\n",
        "| Library                           | Handling of Missing Values                                          |\n",
        "| --------------------------------- | ------------------------------------------------------------------- |\n",
        "| `scikit-learn`                    | Requires **manual imputation** (no native handling)                 |\n",
        "| `XGBoost`, `LightGBM`, `CatBoost` | Handle missing values **natively** using learned default directions |\n",
        "| `rpart` (R)                       | Uses **surrogate splits**                                           |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Summary**\n",
        "\n",
        "| Strategy                | Description                                                   |\n",
        "| ----------------------- | ------------------------------------------------------------- |\n",
        "| Surrogate Splits        | Use alternative features for routing missing data             |\n",
        "| Proportional Assignment | Send missing samples down all branches based on probabilities |\n",
        "| Imputation              | Fill in missing values before training                        |\n",
        "| Treat as Category       | Treat missing as a distinct category (categorical only)       |\n",
        "\n",
        "👉 Some decision tree implementations handle missing values **elegantly and automatically**, while others (like scikit-learn) require **explicit preprocessing**.\n"
      ],
      "metadata": {
        "id": "vYlTGIbXthAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14.  How does a Decision Tree handle categorical features?**"
      ],
      "metadata": {
        "id": "vqEaPQhvn0cG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** Decision Trees can handle **categorical features** in various ways depending on the algorithm and implementation. Here's a breakdown of how they do it:\n",
        "\n",
        "---\n",
        "\n",
        "## 🌳 **How Decision Trees Handle Categorical Features**\n",
        "\n",
        "### 1. **Native Support (Some Algorithms)**\n",
        "\n",
        "Certain decision tree algorithms **natively handle categorical variables** without needing encoding.\n",
        "\n",
        "* They treat each category as a discrete branch or consider subsets of categories to split on.\n",
        "\n",
        "**Examples of algorithms with native support:**\n",
        "\n",
        "* `C4.5`, `C5.0`\n",
        "* `CART` (in some implementations like `rpart` in R)\n",
        "* `CatBoost` (optimized for categorical features)\n",
        "* `LightGBM` (if you specify categorical features)\n",
        "\n",
        "---\n",
        "\n",
        "## 🔀 **2. Manual Encoding (Required in Some Libraries)**\n",
        "\n",
        "### In libraries like **scikit-learn**, you must **manually convert** categorical features to numeric:\n",
        "\n",
        "#### a. **One-Hot Encoding**\n",
        "\n",
        "* Each category becomes a separate binary feature.\n",
        "* Example: `\"Color\" = [Red, Green, Blue]` becomes three features: `[is_red, is_green, is_blue]`.\n",
        "* Decision trees handle this well since they naturally select the most informative binary splits.\n",
        "\n",
        "#### b. **Ordinal Encoding (Integer Encoding)**\n",
        "\n",
        "* Categories are replaced by integer codes: `[Red, Green, Blue] → [0, 1, 2]`\n",
        "* ⚠️ Use **only if the categories have meaningful order** — otherwise, it can mislead the model.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 **How Trees Split on Categorical Features**\n",
        "\n",
        "For a categorical feature with values $\\{A, B, C\\}$, a decision tree may split like:\n",
        "\n",
        "$$\n",
        "\\text{Is feature in } \\{A, C\\}?\n",
        "$$\n",
        "\n",
        "* If **yes**, go left.\n",
        "* If **no** (i.e., it's B), go right.\n",
        "\n",
        "This is known as **subset splitting** and works for both classification and regression tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## 📦 **Handling in Popular Libraries**\n",
        "\n",
        "| Library          | Categorical Support          | Notes                                                |\n",
        "| ---------------- | ---------------------------- | ---------------------------------------------------- |\n",
        "| **scikit-learn** | ❌ (requires encoding)        | Use One-Hot or Ordinal Encoding                      |\n",
        "| **LightGBM**     | ✅ Native support             | Specify `categorical_feature` parameter              |\n",
        "| **CatBoost**     | ✅ Strong native support      | Automatically detects and processes categorical data |\n",
        "| **XGBoost**      | ⚠️ Partial (recent versions) | Some native support, but encoding often still needed |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Summary**\n",
        "\n",
        "| Method           | Description                                                  |\n",
        "| ---------------- | ------------------------------------------------------------ |\n",
        "| Native Handling  | Directly splits on categories or category subsets            |\n",
        "| One-Hot Encoding | Converts categories to binary features (common + robust)     |\n",
        "| Ordinal Encoding | Assigns integers to categories (only if order matters)       |\n",
        "| Subset Splits    | Splits on groups of categories (e.g., `Color ∈ {Red, Blue}`) |\n",
        "\n",
        "> 🎯 **Best Practice:** If your tree implementation **doesn't support categoricals**, use **one-hot encoding** for safe, interpretable splits. If it **does**, take advantage of native handling for better performance and simplicity.\n"
      ],
      "metadata": {
        "id": "wQOosB47txnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15. What are some real-world applications of Decision Trees?**"
      ],
      "metadata": {
        "id": "Kg7RYVPQn5yG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans:** Decision Trees are widely used in the real world due to their simplicity, interpretability, and flexibility. Below are several **real-world applications** across various domains:\n",
        "\n",
        "---\n",
        "\n",
        "## 🏥 1. **Healthcare and Medical Diagnosis**\n",
        "\n",
        "* **Use Case:** Predicting whether a patient has a certain disease based on symptoms, test results, or history.\n",
        "* **Example:** Classifying patients into “low risk” and “high risk” for heart disease.\n",
        "* **Why Trees?** Easy to explain decisions to medical professionals.\n",
        "\n",
        "---\n",
        "\n",
        "## 💳 2. **Finance and Credit Scoring**\n",
        "\n",
        "* **Use Case:** Approving or denying credit card or loan applications.\n",
        "* **Example:** Predicting loan default risk based on income, credit history, etc.\n",
        "* **Why Trees?** Interpretability is essential for regulatory compliance.\n",
        "\n",
        "---\n",
        "\n",
        "## 🛒 3. **Retail and Customer Behavior**\n",
        "\n",
        "* **Use Case:** Recommending products or predicting customer churn.\n",
        "* **Example:** Classifying customers likely to cancel subscriptions.\n",
        "* **Why Trees?** Can easily model non-linear customer behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 4. **Education and Student Performance**\n",
        "\n",
        "* **Use Case:** Predicting student drop-out or academic success.\n",
        "* **Example:** Using attendance, GPA, and socioeconomic background to forecast graduation likelihood.\n",
        "* **Why Trees?** Easy to communicate with educators and policymakers.\n",
        "\n",
        "---\n",
        "\n",
        "## 🚗 5. **Manufacturing and Quality Control**\n",
        "\n",
        "* **Use Case:** Detecting defects or predicting machine failure.\n",
        "* **Example:** Classifying parts as defective based on measurements.\n",
        "* **Why Trees?** Real-time decision rules based on sensor data.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏦 6. **Fraud Detection**\n",
        "\n",
        "* **Use Case:** Identifying potentially fraudulent transactions.\n",
        "* **Example:** Decision Trees or ensembles flag unusual transaction patterns.\n",
        "* **Why Trees?** Fast inference and explainable decisions.\n",
        "\n",
        "---\n",
        "\n",
        "## 🌍 7. **Environmental Science**\n",
        "\n",
        "* **Use Case:** Predicting weather patterns or pollution levels.\n",
        "* **Example:** Using temperature, humidity, and wind to forecast air quality.\n",
        "* **Why Trees?** Can model complex interactions between features.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧾 8. **Marketing and Lead Scoring**\n",
        "\n",
        "* **Use Case:** Ranking leads by likelihood to convert.\n",
        "* **Example:** Decision Trees used to classify users who are likely to click or purchase.\n",
        "* **Why Trees?** Helps prioritize sales efforts efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "## 📷 9. **Image Recognition (with Decision Tree Ensembles)**\n",
        "\n",
        "* **Use Case:** Used in models like Random Forests for classifying images.\n",
        "* **Example:** Recognizing handwritten digits (e.g., MNIST).\n",
        "* **Why Trees?** Often used as a base learner in ensemble methods for vision tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Summary Table\n",
        "\n",
        "| Domain        | Example Application                               |\n",
        "| ------------- | ------------------------------------------------- |\n",
        "| Healthcare    | Disease prediction, patient risk scoring          |\n",
        "| Finance       | Credit scoring, loan approval, fraud detection    |\n",
        "| Retail        | Customer churn, product recommendations           |\n",
        "| Education     | Dropout prediction, performance forecasting       |\n",
        "| Manufacturing | Defect detection, maintenance prediction          |\n",
        "| Marketing     | Lead scoring, campaign targeting                  |\n",
        "| Environment   | Air quality or weather prediction                 |\n",
        "| AI/Vision     | Part of ensemble methods for image classification |\n",
        "\n",
        "---\n",
        "\n",
        "> 🧩 **Note:** While Decision Trees are powerful alone, they become even more effective when used in **ensemble methods** like **Random Forests**, **Gradient Boosting**, and **XGBoost**—which are widely used in **industry and competitions (e.g., Kaggle)**.\n"
      ],
      "metadata": {
        "id": "xgfC9_p6t4-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**PRACTICAL**"
      ],
      "metadata": {
        "id": "-gvQrnSwuL6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy.**"
      ],
      "metadata": {
        "id": "c2fPEhQtuPyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data      # Features\n",
        "y = iris.target    # Labels\n",
        "\n",
        "# Step 2: Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize and train the Decision Tree Classifier\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Classifier Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fdIifByvmHG",
        "outputId": "6f0d0ce0-e12b-448f-f5b5-e081c0863575"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the**\n",
        "**feature importances.**"
      ],
      "metadata": {
        "id": "IC1wVYX_uWre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Step 2: Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Create and train the Decision Tree using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVtVZh97v8fl",
        "outputId": "dcf241a9-7468-4e22-8527-edc6e335faad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the**\n",
        "**model accuracy.**"
      ],
      "metadata": {
        "id": "Stce95kWubTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Initialize the Decision Tree Classifier using 'entropy'\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Classifier Accuracy (Entropy): {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3n487-hwDts",
        "outputId": "8441d733-65e9-4c36-dd76-ed6a2aecc0a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier Accuracy (Entropy): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19.Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean**\n",
        "**Squared Error (MSE).**"
      ],
      "metadata": {
        "id": "mEK_fug6uhki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (Decision Tree Regressor): {mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjwAc-bowMey",
        "outputId": "522f6c71-06c5-4b1e-c013-e4eff7e7e2e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Decision Tree Regressor): 0.4952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz.**"
      ],
      "metadata": {
        "id": "kt85uqCSuoz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y graphviz\n",
        "!pip install graphviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1zg2tiGw_vz",
        "outputId": "d9a2d556-2b83-42b8-d8c7-4b241b8aefb9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "graphviz is already the newest version (2.42.2-6ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train a Decision Tree\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Export the tree to DOT format\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "# Visualize in Colab using graphviz\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"iris_decision_tree\", format=\"png\")  # Saves to file\n",
        "graph  # Displays in Colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "mbYwZkK0xD2V",
        "outputId": "1f587ecf-c8ac-4cbc-f0dd-77914f8b1de3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"863pt\" height=\"671pt\"\n viewBox=\"0.00 0.00 863.00 671.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 667)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-667 859,-667 859,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M517,-663C517,-663 382,-663 382,-663 376,-663 370,-657 370,-651 370,-651 370,-592 370,-592 370,-586 376,-580 382,-580 382,-580 517,-580 517,-580 523,-580 529,-586 529,-592 529,-592 529,-651 529,-651 529,-657 523,-663 517,-663\"/>\n<text text-anchor=\"start\" x=\"378\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 2.45</text>\n<text text-anchor=\"start\" x=\"414\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n<text text-anchor=\"start\" x=\"404.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 150</text>\n<text text-anchor=\"start\" x=\"391.5\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 50, 50]</text>\n<text text-anchor=\"start\" x=\"406\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M419,-536.5C419,-536.5 326,-536.5 326,-536.5 320,-536.5 314,-530.5 314,-524.5 314,-524.5 314,-480.5 314,-480.5 314,-474.5 320,-468.5 326,-468.5 326,-468.5 419,-468.5 419,-468.5 425,-468.5 431,-474.5 431,-480.5 431,-480.5 431,-524.5 431,-524.5 431,-530.5 425,-536.5 419,-536.5\"/>\n<text text-anchor=\"start\" x=\"344.5\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"331.5\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n<text text-anchor=\"start\" x=\"322\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 0, 0]</text>\n<text text-anchor=\"start\" x=\"329\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M422.79,-579.91C415.38,-568.65 407.33,-556.42 399.88,-545.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"402.75,-543.1 394.33,-536.67 396.9,-546.94 402.75,-543.1\"/>\n<text text-anchor=\"middle\" x=\"389.28\" y=\"-557.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M591.5,-544C591.5,-544 461.5,-544 461.5,-544 455.5,-544 449.5,-538 449.5,-532 449.5,-532 449.5,-473 449.5,-473 449.5,-467 455.5,-461 461.5,-461 461.5,-461 591.5,-461 591.5,-461 597.5,-461 603.5,-467 603.5,-473 603.5,-473 603.5,-532 603.5,-532 603.5,-538 597.5,-544 591.5,-544\"/>\n<text text-anchor=\"start\" x=\"457.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n<text text-anchor=\"start\" x=\"498.5\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"481.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n<text text-anchor=\"start\" x=\"472\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 50, 50]</text>\n<text text-anchor=\"start\" x=\"474\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M476.21,-579.91C482.01,-571.1 488.2,-561.7 494.18,-552.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"497.26,-554.3 499.83,-544.02 491.41,-550.45 497.26,-554.3\"/>\n<text text-anchor=\"middle\" x=\"504.88\" y=\"-564.81\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#4de88e\" stroke=\"black\" d=\"M482,-425C482,-425 347,-425 347,-425 341,-425 335,-419 335,-413 335,-413 335,-354 335,-354 335,-348 341,-342 347,-342 347,-342 482,-342 482,-342 488,-342 494,-348 494,-354 494,-354 494,-413 494,-413 494,-419 488,-425 482,-425\"/>\n<text text-anchor=\"start\" x=\"343\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\n<text text-anchor=\"start\" x=\"379\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.168</text>\n<text text-anchor=\"start\" x=\"373.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 54</text>\n<text text-anchor=\"start\" x=\"364\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 49, 5]</text>\n<text text-anchor=\"start\" x=\"362\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M487.64,-460.91C478.87,-451.74 469.47,-441.93 460.44,-432.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"462.73,-429.82 453.29,-425.02 457.68,-434.66 462.73,-429.82\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#843de6\" stroke=\"black\" d=\"M706,-425C706,-425 571,-425 571,-425 565,-425 559,-419 559,-413 559,-413 559,-354 559,-354 559,-348 565,-342 571,-342 571,-342 706,-342 706,-342 712,-342 718,-348 718,-354 718,-354 718,-413 718,-413 718,-419 712,-425 706,-425\"/>\n<text text-anchor=\"start\" x=\"567\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n<text text-anchor=\"start\" x=\"603\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.043</text>\n<text text-anchor=\"start\" x=\"597.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\n<text text-anchor=\"start\" x=\"588\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 45]</text>\n<text text-anchor=\"start\" x=\"590\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>2&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M565.36,-460.91C574.13,-451.74 583.53,-441.93 592.56,-432.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"595.32,-434.66 599.71,-425.02 590.27,-429.82 595.32,-434.66\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#3de684\" stroke=\"black\" d=\"M260.5,-306C260.5,-306 130.5,-306 130.5,-306 124.5,-306 118.5,-300 118.5,-294 118.5,-294 118.5,-235 118.5,-235 118.5,-229 124.5,-223 130.5,-223 130.5,-223 260.5,-223 260.5,-223 266.5,-223 272.5,-229 272.5,-235 272.5,-235 272.5,-294 272.5,-294 272.5,-300 266.5,-306 260.5,-306\"/>\n<text text-anchor=\"start\" x=\"126.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.65</text>\n<text text-anchor=\"start\" x=\"160\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.041</text>\n<text text-anchor=\"start\" x=\"154.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 48</text>\n<text text-anchor=\"start\" x=\"145\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 47, 1]</text>\n<text text-anchor=\"start\" x=\"143\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M338.52,-341.91C319.75,-331.88 299.52,-321.07 280.36,-310.84\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"281.82,-307.65 271.35,-306.02 278.52,-313.82 281.82,-307.65\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M479.5,-306C479.5,-306 349.5,-306 349.5,-306 343.5,-306 337.5,-300 337.5,-294 337.5,-294 337.5,-235 337.5,-235 337.5,-229 343.5,-223 349.5,-223 349.5,-223 479.5,-223 479.5,-223 485.5,-223 491.5,-229 491.5,-235 491.5,-235 491.5,-294 491.5,-294 491.5,-300 485.5,-306 479.5,-306\"/>\n<text text-anchor=\"start\" x=\"345.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.55</text>\n<text text-anchor=\"start\" x=\"379\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"377\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"367.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n<text text-anchor=\"start\" x=\"366\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>3&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M414.5,-341.91C414.5,-333.65 414.5,-324.86 414.5,-316.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"418,-316.02 414.5,-306.02 411,-316.02 418,-316.02\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-179.5C109,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 109,-111.5 109,-111.5 115,-111.5 121,-117.5 121,-123.5 121,-123.5 121,-167.5 121,-167.5 121,-173.5 115,-179.5 109,-179.5\"/>\n<text text-anchor=\"start\" x=\"32.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 47</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 47, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 4&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M148.66,-222.91C135.04,-211.1 120.17,-198.22 106.6,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"108.62,-183.57 98.77,-179.67 104.03,-188.86 108.62,-183.57\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-179.5C240,-179.5 151,-179.5 151,-179.5 145,-179.5 139,-173.5 139,-167.5 139,-167.5 139,-123.5 139,-123.5 139,-117.5 145,-111.5 151,-111.5 151,-111.5 240,-111.5 240,-111.5 246,-111.5 252,-117.5 252,-123.5 252,-123.5 252,-167.5 252,-167.5 252,-173.5 246,-179.5 240,-179.5\"/>\n<text text-anchor=\"start\" x=\"167.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 4&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>4&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M195.5,-222.91C195.5,-212.2 195.5,-200.62 195.5,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"199,-189.67 195.5,-179.67 192,-189.67 199,-189.67\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M371,-179.5C371,-179.5 282,-179.5 282,-179.5 276,-179.5 270,-173.5 270,-167.5 270,-167.5 270,-123.5 270,-123.5 270,-117.5 276,-111.5 282,-111.5 282,-111.5 371,-111.5 371,-111.5 377,-111.5 383,-117.5 383,-123.5 383,-123.5 383,-167.5 383,-167.5 383,-173.5 377,-179.5 371,-179.5\"/>\n<text text-anchor=\"start\" x=\"298.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"289\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"279.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n<text text-anchor=\"start\" x=\"278\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M383.97,-222.91C375.42,-211.54 366.12,-199.18 357.54,-187.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"360.25,-185.55 351.45,-179.67 354.66,-189.76 360.25,-185.55\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M548,-187C548,-187 413,-187 413,-187 407,-187 401,-181 401,-175 401,-175 401,-116 401,-116 401,-110 407,-104 413,-104 413,-104 548,-104 548,-104 554,-104 560,-110 560,-116 560,-116 560,-175 560,-175 560,-181 554,-187 548,-187\"/>\n<text text-anchor=\"start\" x=\"409\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.45</text>\n<text text-anchor=\"start\" x=\"445\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"443\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"433.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n<text text-anchor=\"start\" x=\"428\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M437.4,-222.91C442.31,-214.2 447.56,-204.9 452.64,-195.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"455.78,-197.45 457.64,-187.02 449.68,-194.01 455.78,-197.45\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M461,-68C461,-68 364,-68 364,-68 358,-68 352,-62 352,-56 352,-56 352,-12 352,-12 352,-6 358,0 364,0 364,0 461,0 461,0 467,0 473,-6 473,-12 473,-12 473,-56 473,-56 473,-62 467,-68 461,-68\"/>\n<text text-anchor=\"start\" x=\"384.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"375\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"365.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"360\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M455.18,-103.73C449.74,-94.97 443.99,-85.7 438.52,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"441.43,-74.95 433.18,-68.3 435.48,-78.64 441.43,-74.95\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M592,-68C592,-68 503,-68 503,-68 497,-68 491,-62 491,-56 491,-56 491,-12 491,-12 491,-6 497,0 503,0 503,0 592,0 592,0 598,0 604,-6 604,-12 604,-12 604,-56 604,-56 604,-62 598,-68 592,-68\"/>\n<text text-anchor=\"start\" x=\"519.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"510\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"500.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"499\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M505.45,-103.73C510.81,-94.97 516.48,-85.7 521.86,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"524.89,-78.66 527.12,-68.3 518.92,-75 524.89,-78.66\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M707.5,-306C707.5,-306 569.5,-306 569.5,-306 563.5,-306 557.5,-300 557.5,-294 557.5,-294 557.5,-235 557.5,-235 557.5,-229 563.5,-223 569.5,-223 569.5,-223 707.5,-223 707.5,-223 713.5,-223 719.5,-229 719.5,-235 719.5,-235 719.5,-294 719.5,-294 719.5,-300 713.5,-306 707.5,-306\"/>\n<text text-anchor=\"start\" x=\"565.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal length (cm) ≤ 5.95</text>\n<text text-anchor=\"start\" x=\"603\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"601\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"591.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n<text text-anchor=\"start\" x=\"590\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 12&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>12&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M638.5,-341.91C638.5,-333.65 638.5,-324.86 638.5,-316.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"642,-316.02 638.5,-306.02 635,-316.02 642,-316.02\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M843,-298.5C843,-298.5 750,-298.5 750,-298.5 744,-298.5 738,-292.5 738,-286.5 738,-286.5 738,-242.5 738,-242.5 738,-236.5 744,-230.5 750,-230.5 750,-230.5 843,-230.5 843,-230.5 849,-230.5 855,-236.5 855,-242.5 855,-242.5 855,-286.5 855,-286.5 855,-292.5 849,-298.5 843,-298.5\"/>\n<text text-anchor=\"start\" x=\"768.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"755.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n<text text-anchor=\"start\" x=\"746\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 43]</text>\n<text text-anchor=\"start\" x=\"748\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 12&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>12&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M693.32,-341.91C709.56,-329.88 727.31,-316.73 743.44,-304.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"745.76,-307.43 751.71,-298.67 741.59,-301.81 745.76,-307.43\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M687,-179.5C687,-179.5 590,-179.5 590,-179.5 584,-179.5 578,-173.5 578,-167.5 578,-167.5 578,-123.5 578,-123.5 578,-117.5 584,-111.5 590,-111.5 590,-111.5 687,-111.5 687,-111.5 693,-111.5 699,-117.5 699,-123.5 699,-123.5 699,-167.5 699,-167.5 699,-173.5 693,-179.5 687,-179.5\"/>\n<text text-anchor=\"start\" x=\"610.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"601\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"591.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n<text text-anchor=\"start\" x=\"586\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 13&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>13&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M638.5,-222.91C638.5,-212.2 638.5,-200.62 638.5,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"642,-189.67 638.5,-179.67 635,-189.67 642,-189.67\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M818,-179.5C818,-179.5 729,-179.5 729,-179.5 723,-179.5 717,-173.5 717,-167.5 717,-167.5 717,-123.5 717,-123.5 717,-117.5 723,-111.5 729,-111.5 729,-111.5 818,-111.5 818,-111.5 824,-111.5 830,-117.5 830,-123.5 830,-123.5 830,-167.5 830,-167.5 830,-173.5 824,-179.5 818,-179.5\"/>\n<text text-anchor=\"start\" x=\"745.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"736\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"726.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n<text text-anchor=\"start\" x=\"725\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 13&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>13&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M685.34,-222.91C698.96,-211.1 713.83,-198.22 727.4,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"729.97,-188.86 735.23,-179.67 725.38,-183.57 729.97,-188.86\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7b4b27c71b10>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its**\n",
        "**accuracy with a fully grown tree.**"
      ],
      "metadata": {
        "id": "4xOHq4HDuvsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Decision Tree with max depth of 3 (pruned)\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "y_pred_pruned = pruned_tree.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# Step 4: Train fully grown Decision Tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Step 5: Print the comparison\n",
        "print(f\"Pruned Tree Accuracy (max_depth=3): {accuracy_pruned:.2f}\")\n",
        "print(f\"Full Tree Accuracy (no max depth): {accuracy_full:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH_qsulIzS1b",
        "outputId": "873429b6-26ce-467a-efdf-316e326f5122"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned Tree Accuracy (max_depth=3): 1.00\n",
            "Full Tree Accuracy (no max depth): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its**\n",
        "**accuracy with a default tree.**"
      ],
      "metadata": {
        "id": "3Ql_Cww1u0jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Decision Tree with min_samples_split=5\n",
        "tree_split_5 = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "tree_split_5.fit(X_train, y_train)\n",
        "y_pred_split_5 = tree_split_5.predict(X_test)\n",
        "accuracy_split_5 = accuracy_score(y_test, y_pred_split_5)\n",
        "\n",
        "# Step 4: Train default Decision Tree\n",
        "default_tree = DecisionTreeClassifier(random_state=42)\n",
        "default_tree.fit(X_train, y_train)\n",
        "y_pred_default = default_tree.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Step 5: Print the comparison\n",
        "print(f\"Tree with min_samples_split=5 Accuracy: {accuracy_split_5:.2f}\")\n",
        "print(f\"Default Tree Accuracy: {accuracy_default:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB29vSAHzi88",
        "outputId": "65cfada9-228d-45e3-e5ee-c7dc1f35ce26"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tree with min_samples_split=5 Accuracy: 1.00\n",
            "Default Tree Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its**\n",
        "**accuracy with unscaled data.**"
      ],
      "metadata": {
        "id": "5o3XSmHbu66C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train a Decision Tree WITHOUT scaling\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Step 4: Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Train a Decision Tree WITH scaled data\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 6: Compare results\n",
        "print(f\"Accuracy WITHOUT Scaling: {accuracy_unscaled:.2f}\")\n",
        "print(f\"Accuracy WITH Scaling   : {accuracy_scaled:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5sjn5BSzwym",
        "outputId": "92aa6699-cf5c-43c1-a278-e1b4d5e55db6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Scaling: 1.00\n",
            "Accuracy WITH Scaling   : 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q24.Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass**\n",
        "**classification.**"
      ],
      "metadata": {
        "id": "mI2-evzqu_5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Wrap DecisionTreeClassifier with One-vs-Rest strategy\n",
        "ovr_classifier = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = ovr_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Decision Tree Classifier with One-vs-Rest Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PVQdSRQz7h9",
        "outputId": "18f63876-4300-41ae-a7de-54c2cd2ab5ff"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier with One-vs-Rest Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores.**"
      ],
      "metadata": {
        "id": "v8dyU_qevEb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Get feature importance scores\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Step 5: Display feature importances\n",
        "importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance Score\": importances\n",
        "}).sort_values(by=\"Importance Score\", ascending=False)\n",
        "\n",
        "print(\"Feature Importance Scores:\")\n",
        "print(importance_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP3PvprJ0O64",
        "outputId": "f5df68f0-c464-4099-c607-f6075f509bed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance Scores:\n",
            "             Feature  Importance Score\n",
            "2  petal length (cm)          0.906143\n",
            "3   petal width (cm)          0.077186\n",
            "1   sepal width (cm)          0.016670\n",
            "0  sepal length (cm)          0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q26.Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance**\n",
        "**with an unrestricted tree.**"
      ],
      "metadata": {
        "id": "KLKJvgalvI-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Decision Tree Regressor with max_depth=5\n",
        "regressor_pruned = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "regressor_pruned.fit(X_train, y_train)\n",
        "y_pred_pruned = regressor_pruned.predict(X_test)\n",
        "mse_pruned = mean_squared_error(y_test, y_pred_pruned)\n",
        "\n",
        "# Step 4: Train unrestricted Decision Tree Regressor\n",
        "regressor_full = DecisionTreeRegressor(random_state=42)\n",
        "regressor_full.fit(X_train, y_train)\n",
        "y_pred_full = regressor_full.predict(X_test)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "\n",
        "# Step 5: Display the results\n",
        "print(f\"Decision Tree Regressor with max_depth=5 MSE: {mse_pruned:.4f}\")\n",
        "print(f\"Unrestricted Decision Tree Regressor MSE  : {mse_full:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qllHqTS20YhT",
        "outputId": "bc38bd12-d4f2-4c67-a7c8-37374d659fd3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor with max_depth=5 MSE: 0.5245\n",
            "Unrestricted Decision Tree Regressor MSE  : 0.4952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and**\n",
        "**visualize its effect on accuracy.**"
      ],
      "metadata": {
        "id": "pDbzRXO3vON6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train initial tree to obtain effective alphas\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "# Step 4: Train trees for each ccp_alpha\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "\n",
        "# Step 5: Remove last tree if it's trivial\n",
        "clfs = clfs[:-1]\n",
        "ccp_alphas = ccp_alphas[:-1]\n",
        "\n",
        "# Step 6: Compute train and test accuracy\n",
        "train_accuracies = [accuracy_score(y_train, clf.predict(X_train)) for clf in clfs]\n",
        "test_accuracies = [accuracy_score(y_test, clf.predict(X_test)) for clf in clfs]\n",
        "\n",
        "# Step 7: Plot accuracy vs. ccp_alpha\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ccp_alphas, train_accuracies, marker='o', label=\"Train Accuracy\", drawstyle=\"steps-post\")\n",
        "plt.plot(ccp_alphas, test_accuracies, marker='o', label=\"Test Accuracy\", drawstyle=\"steps-post\")\n",
        "plt.xlabel(\"ccp_alpha (complexity)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Cost Complexity Pruning on Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "fmYszBt40hbv",
        "outputId": "08c36e45-578f-4b8d-f690-ad2a30ffa1b8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,**\n",
        "**Recall, and F1-Score.**"
      ],
      "metadata": {
        "id": "SmPRinFOvS0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate using classification report\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ETFoYBA0pP-",
        "outputId": "51624d2c-2265-442d-ed64-18ed8bea205b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q29.Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn.**"
      ],
      "metadata": {
        "id": "9W62ZJjtvYgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Decision Tree\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 5: Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 6: Visualize using Seaborn heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "ByGPlgBp0_VJ",
        "outputId": "4ace690e-ac96-4900-8164-96ff6c384afa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values**\n",
        "**for max_depth and min_samples_split.**"
      ],
      "metadata": {
        "id": "10ZfGtFMvfyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Set up the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Step 4: Initialize the Decision Tree and GridSearchCV\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Step 5: Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict and evaluate\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Display results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Test Set Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VK_o_GM1H2O",
        "outputId": "7f58c3f9-a4a0-4172-fa16-ca4a701b4913"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Test Set Accuracy: 1.00\n"
          ]
        }
      ]
    }
  ]
}